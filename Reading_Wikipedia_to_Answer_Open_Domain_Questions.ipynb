{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajabia/LLM_codes/blob/main/Reading_Wikipedia_to_Answer_Open_Domain_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwDRwge3EVSq"
      },
      "outputs": [],
      "source": [
        "#https://github.com/kushalj001/pytorch-question-answering/blob/master/1.%20DrQA.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PmTAe4xVEf8D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchtext\n",
        "import torch\n",
        "from torch import nn\n",
        "import json, re, unicodedata, string, typing, time\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import pickle\n",
        "from nltk import word_tokenize\n",
        "# nlp = spacy.load('en')\n",
        "# from preprocess import *\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1J29qz-FYza",
        "outputId": "cee653a8-88e3-421a-d3e3-affd989d2ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "drive  sample_data\n",
            "'Arezoo Rajabi-local community detection-Chaos paper.rar'   qanetc2id.pickle   qanetw2id.pickle\n",
            " preprocess.py\t\t\t\t\t\t    qanettrain.pkl     squad_dev.json\n",
            " __pycache__\t\t\t\t\t\t    qanetvalid.pkl     squad_train.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls\n",
        "!ls /content/drive/MyDrive/GoogleColab/QAWiki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rED3vHvXHVZ4",
        "outputId": "382497f0-66ac-4cc8-dc5b-c4634d43312c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data:  442\n",
            "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
            "Title:  University_of_Notre_Dame\n",
            "Length of data:  48\n",
            "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
            "Title:  Super_Bowl_50\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/GoogleColab/QAWiki')\n",
        "from preprocess import *\n",
        "train_data = load_json('/content/drive/MyDrive/GoogleColab/QAWiki/squad_train.json')\n",
        "valid_data = load_json('/content/drive/MyDrive/GoogleColab/QAWiki/squad_dev.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sjHWHbpIM1q"
      },
      "outputs": [],
      "source": [
        "train_list = parse_data(train_data)\n",
        "valid_list = parse_data(valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv_Drq1UcX2i",
        "outputId": "1801cc4a-5c32-4dc1-b968-57901ccff4db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train list len:  87599\n",
            "Valid list len:  34726\n",
            "Train list:  {'id': '5733be284776f41900661182', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'label': [515, 541], 'answer': 'Saint Bernadette Soubirous'}\n",
            "Valid list:  {'id': '56be4db0acb8001400a502ec', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team represented the AFC at Super Bowl 50?', 'label': [177, 191], 'answer': 'Denver Broncos'}\n",
            "dict_keys(['id', 'context', 'question', 'label', 'answer'])\n"
          ]
        }
      ],
      "source": [
        "print('Train list len: ',len(train_list))\n",
        "print('Valid list len: ',len(valid_list))\n",
        "print('Train list: ',train_list[0])\n",
        "print('Valid list: ',valid_list[0])\n",
        "print(train_list[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRXbGiNgdERE",
        "outputId": "a5c3b49f-c3c9-4703-e8da-64bf3e064a73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         id  \\\n",
            "0  5733be284776f41900661182   \n",
            "1  5733be284776f4190066117f   \n",
            "\n",
            "                                             context  \\\n",
            "0  Architecturally, the school has a Catholic cha...   \n",
            "1  Architecturally, the school has a Catholic cha...   \n",
            "\n",
            "                                            question       label  \\\n",
            "0  To whom did the Virgin Mary allegedly appear i...  [515, 541]   \n",
            "1  What is in front of the Notre Dame Main Building?  [188, 213]   \n",
            "\n",
            "                       answer  \n",
            "0  Saint Bernadette Soubirous  \n",
            "1   a copper statue of Christ  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.DataFrame(train_list)\n",
        "valid_df = pd.DataFrame(valid_list)\n",
        "print(train_df.head(2))\n",
        "# get indices of outliers and drop them from the dataframe\n",
        "\n",
        "drop_ids_train = filter_large_examples(train_df)\n",
        "train_df.drop(list(drop_ids_train), inplace=True)\n",
        "\n",
        "drop_ids_valid = filter_large_examples(valid_df)\n",
        "valid_df.drop(list(drop_ids_valid), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kizv1MbdbfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f00d79-f111-4103-f02f-6b330d69aab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in the dataset:  118427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw-vocab: 109850\n",
            "vocab-length: 109852\n",
            "word2idx-length: 109852\n",
            "CPU times: user 8min 30s, sys: 2.22 s, total: 8min 32s\n",
            "Wall time: 8min 36s\n",
            "----------------------------------\n",
            "raw-char-vocab: 1401\n",
            "char-vocab-intersect: 232\n",
            "char2idx-length: 234\n",
            "CPU times: user 2.4 s, sys: 89.9 ms, total: 2.49 s\n",
            "Wall time: 2.49 s\n",
            "CPU times: user 14min 25s, sys: 2.84 s, total: 14min 28s\n",
            "Wall time: 14min 32s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5min 44s, sys: 1.15 s, total: 5min 45s\n",
            "Wall time: 5min 47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 36s, sys: 1.21 s, total: 4min 37s\n",
            "Wall time: 4min 38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 49s, sys: 448 ms, total: 1min 49s\n",
            "Wall time: 1min 50s\n"
          ]
        }
      ],
      "source": [
        "vocab_text = gather_text_for_vocab([train_df, valid_df])\n",
        "print(\"Number of sentences in the dataset: \", len(vocab_text))\n",
        "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)\n",
        "print(\"----------------------------------\")\n",
        "%time char2idx, char_vocab = build_char_vocab(vocab_text)\n",
        "\n",
        "%time train_df['context_ids'] = train_df.context.apply(context_to_ids, word2idx=word2idx)\n",
        "%time valid_df['context_ids'] = valid_df.context.apply(context_to_ids, word2idx=word2idx)\n",
        "%time train_df['question_ids'] = train_df.question.apply(question_to_ids, word2idx=word2idx)\n",
        "%time valid_df['question_ids'] = valid_df.question.apply(question_to_ids, word2idx=word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jve-l-sFeLMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47de4e5e-e9c5-4999-e646-d96bac1d17eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of error indices: 921\n",
            "Number of error indices: 359\n",
            "86403 34080\n"
          ]
        }
      ],
      "source": [
        "# get indices with tokenization errors and drop those indices\n",
        "\n",
        "train_err = get_error_indices(train_df, idx2word)\n",
        "valid_err = get_error_indices(valid_df, idx2word)\n",
        "\n",
        "train_df.drop(train_err, inplace=True)\n",
        "valid_df.drop(valid_err, inplace=True)\n",
        "print(len(train_df), len(valid_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNSQ68Jea3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98eb50d5-3036-4113-e1a7-d6fefa92f3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "# get start and end positions of answers from the context\n",
        "# this is basically the label for training QA models\n",
        "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
        "valid_label_idx = valid_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
        "\n",
        "train_df['label_idx'] = train_label_idx\n",
        "valid_df['label_idx'] = valid_label_idx\n",
        "\n",
        "print(train_df.head(2))\n",
        "folder='/content/drive/MyDrive/GoogleColab/QAWiki/'\n",
        "train_df.to_pickle('qanettrain.pkl')\n",
        "valid_df.to_pickle('qanetvalid.pkl')\n",
        "\n",
        "import pickle\n",
        "with open(folder+'qanetw2id.pickle','wb') as handle:\n",
        "    pickle.dump(word2idx, handle)\n",
        "\n",
        "with open(folder+'qanetc2id.pickle','wb') as handle:\n",
        "    pickle.dump(char2idx, handle)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder='/content/drive/MyDrive/GoogleColab/QAWiki/'\n",
        "train_df = pd.read_pickle(folder+'qanettrain.pkl')\n",
        "valid_df = pd.read_pickle(folder+'qanetvalid.pkl')\n",
        "\n",
        "word2idx = pd.read_pickle(folder+'qanetw2id.pickle')\n",
        "char2idx = pd.read_pickle(folder+'qanetc2id.pickle')\n",
        "\n",
        "\n",
        "idx2word = {v:k for k,v in word2idx.items()}\n",
        "print(idx2word[103])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34KebTLC5kDs",
        "outputId": "50fdd10c-58d7-4258-82dd-631e136f2ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SquadDataset:\n",
        "    '''\n",
        "    - Creates batches dynamically by padding to the length of largest example\n",
        "      in a given batch.\n",
        "    - Calulates character vectors for contexts and question.\n",
        "    - Returns tensors for training.\n",
        "    '''\n",
        "    def __init__(self, data, batch_size):\n",
        "        '''\n",
        "        data: dataframe\n",
        "        batch_size: int\n",
        "        '''\n",
        "        self.batch_size = batch_size\n",
        "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
        "        self.data = data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def make_char_vector(self, max_sent_len, sentence, max_word_len=16):\n",
        "\n",
        "        char_vec = torch.zeros(max_sent_len, max_word_len).type(torch.LongTensor)\n",
        "\n",
        "        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner'])):\n",
        "            for j, ch in enumerate(word.text):\n",
        "                if j == max_word_len:\n",
        "                    break\n",
        "                char_vec[i][j] = char2idx.get(ch, 0)\n",
        "\n",
        "        return char_vec\n",
        "\n",
        "    def get_span(self, text):\n",
        "\n",
        "        text = nlp(text, disable=['parser','tagger','ner'])\n",
        "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
        "\n",
        "        return span\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        '''\n",
        "        Creates batches of data and yields them.\n",
        "\n",
        "        Each yield comprises of:\n",
        "        :padded_context: padded tensor of contexts for each batch\n",
        "        :padded_question: padded tensor of questions for each batch\n",
        "        :char_ctx & ques_ctx: character-level ids for context and question\n",
        "        :label: start and end index wrt context_ids\n",
        "        :context_text,answer_text: used while validation to calculate metrics\n",
        "        :ids: question_ids for evaluation\n",
        "        '''\n",
        "\n",
        "        for batch in self.data:\n",
        "\n",
        "            spans = []\n",
        "            ctx_text = []\n",
        "            answer_text = []\n",
        "\n",
        "\n",
        "            for ctx in batch.context:\n",
        "                ctx_text.append(ctx)\n",
        "                spans.append(self.get_span(ctx))\n",
        "\n",
        "            for ans in batch.answer:\n",
        "                answer_text.append(ans)\n",
        "\n",
        "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
        "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
        "\n",
        "            for i, ctx in enumerate(batch.context_ids):\n",
        "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
        "\n",
        "            max_word_ctx = 16\n",
        "\n",
        "            char_ctx = torch.zeros(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n",
        "            for i, context in enumerate(batch.context):\n",
        "                char_ctx[i] = self.make_char_vector(max_context_len, context)\n",
        "\n",
        "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
        "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
        "\n",
        "            for i, ques in enumerate(batch.question_ids):\n",
        "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n",
        "\n",
        "            max_word_ques = 16\n",
        "\n",
        "            char_ques = torch.zeros(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n",
        "            for i, question in enumerate(batch.question):\n",
        "                char_ques[i] = self.make_char_vector(max_question_len, question)\n",
        "\n",
        "\n",
        "            label = torch.LongTensor(list(batch.label_idx))\n",
        "            ids = list(batch.id)\n",
        "\n",
        "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_df,16)\n",
        "valid_dataset = SquadDataset(valid_df,16)"
      ],
      "metadata": {
        "id": "GCu4Qno87sK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = next(iter(train_dataset))\n",
        "for i in range(len(a)):\n",
        "    try:\n",
        "        print(a[i].shape)\n",
        "    except AttributeError:\n",
        "        print(len(a[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdFUai9ME7Go",
        "outputId": "e9aab107-4d4c-41c0-acaf-07b4e9a3e756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 253])\n",
            "torch.Size([16, 16])\n",
            "torch.Size([16, 253, 16])\n",
            "torch.Size([16, 16, 16])\n",
            "torch.Size([16, 2])\n",
            "16\n",
            "16\n",
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_glove_dict():\n",
        "    '''\n",
        "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
        "    keys and their respective pretrained word vectors as values.\n",
        "\n",
        "    '''\n",
        "    glove_dict = {}\n",
        "    with open(folder+\"glove.840B.300d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split(' ')\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "            glove_dict[word] = vector\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    return glove_dict"
      ],
      "metadata": {
        "id": "ZSlMVJpHE-Ui"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1aXYFOHh57Kngq4mDt-BHUTuiPgZXjTIF",
      "authorship_tag": "ABX9TyMTr8LF/DWfNFwfaZ9Z8QKF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}